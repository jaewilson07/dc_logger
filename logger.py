import asyncioimport asyncioimport asyncio

from typing import Optional, List

from typing import Optional, List

from .client.enums import LogLevel

from .client.models import LogEntryfrom typing import Optional, List# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/client/97_EnhancedLogger.ipynb

from .client.correlation import correlation_manager

from .client.exceptions import LogConfigErrorfrom .core.enums import LogLevel

from .config.LogConfig import LogConfig

from .config.LogConfig_Cloud import AzureLogAnalyticsConfigfrom .core.models import LogEntry

from .handlers.base import LogHandler

from .handlers.console import ConsoleHandlerfrom .core.correlation import correlation_manager

from .handlers.file import FileHandler

from .handlers.cloud.datadog import DatadogHandlerfrom .core.exceptions import LogConfigErrorfrom .core.enums import LogLevel__all__ = [

from .handlers.cloud.aws import AWSCloudWatchHandler

from .handlers.cloud.gcp import GCPLoggingHandlerfrom .config.base import LogConfig

from .handlers.cloud.azure import AzureLogAnalyticsHandler

from .config.cloud import AzureLogAnalyticsConfigfrom .core.models import LogEntry    'LogLevel', 'LogConfig', 'LogEntry', 'TracebackDetails', 'get_traceback', 'DomoLogger', 'log_function_call',



class DomoLogger:from .handlers.base import LogHandler

    """Enhanced logger with structured logging and multiple handlers"""

from .handlers.console import ConsoleHandlerfrom .core.correlation import correlation_manager    'LoggingError', 'LogHandlerError', 'LogConfigError', 'LogWriteError', 'LogFlushError',

    def __init__(self, config: LogConfig, app_name: str):

        self.config = configfrom .handlers.file import FileHandler

        self.app_name = app_name

        self.handlers: List[LogHandler] = []from .handlers.cloud.datadog import DatadogHandlerfrom .core.exceptions import LogConfigError    'DatadogLogConfig', 'AWSCloudWatchLogConfig', 'GCPLoggingConfig', 'AzureLogAnalyticsConfig', 'ConsoleLogConfig',

        self.buffer: List[LogEntry] = []

        self.correlation_manager = correlation_managerfrom .handlers.cloud.aws import AWSCloudWatchHandler

        self.flush_task = None

from .handlers.cloud.gcp import GCPLoggingHandlerfrom .config.base import LogConfig    'MultiHandlerLogConfig', 'create_multi_handler_config', 'create_console_file_config', 'create_console_datadog_config',

        # Validate configuration

        config.validate_config()from .handlers.cloud.azure import AzureLogAnalyticsHandler



        # Initialize handlers based on configfrom .config.cloud import AzureLogAnalyticsConfig    'CloudHandler', 'DatadogHandler', 'AWSCloudWatchHandler', 'GCPLoggingHandler', 'AzureLogAnalyticsHandler'

        self._setup_handlers()



        # Try to start background flush task if event loop is available

        try:class DomoLogger:from .handlers.base import LogHandler]

            asyncio.get_running_loop()

            self._start_flush_task()    """Enhanced logger with structured logging and multiple handlers"""

        except RuntimeError:

            # No event loop, task will be started when first log is calledfrom .handlers.console import ConsoleHandler

            pass

    def __init__(self, config: LogConfig, app_name: str):

    def _setup_handlers(self):

        """Setup handlers based on configuration"""        self.config = configfrom .handlers.file import FileHandlerimport datetime as dt

        # Get handler configurations from the config

        handler_configs = self.config.get_handler_configs()        self.app_name = app_name

        

        for handler_config in handler_configs:        self.handlers: List[LogHandler] = []from .handlers.cloud.datadog import DatadogHandlerfrom typing import Optional, List, Dict, Any, Union

            handler_type = handler_config["type"]

            config = handler_config["config"]        self.buffer: List[LogEntry] = []

            cloud_config = handler_config.get("cloud_config")

                    self.correlation_manager = correlation_managerfrom .handlers.cloud.aws import AWSCloudWatchHandlerfrom dataclasses import dataclass, field

            if handler_type == "console":

                self.handlers.append(ConsoleHandler(config))        self.flush_task = None

            elif handler_type == "file":

                self.handlers.append(FileHandler(config))from .handlers.cloud.gcp import GCPLoggingHandlerfrom enum import Enum

            elif handler_type == "cloud":

                if cloud_config:        # Validate configuration

                    cloud_provider = cloud_config.get("cloud_provider")

                    if cloud_provider == "datadog":        config.validate_config()from .handlers.cloud.azure import AzureLogAnalyticsHandlerimport json

                        self.handlers.append(DatadogHandler(config))

                    elif cloud_provider == "aws":

                        self.handlers.append(AWSCloudWatchHandler(config))

                    elif cloud_provider == "gcp":        # Initialize handlers based on configimport uuid

                        self.handlers.append(GCPLoggingHandler(config))

                    elif cloud_provider == "azure":        self._setup_handlers()

                        self.handlers.append(AzureLogAnalyticsHandler(config))

                    else:import asyncio

                        raise LogConfigError(f"Unknown cloud provider: {cloud_provider}")

                else:        # Try to start background flush task if event loop is available

                    raise LogConfigError("Cloud handler missing cloud_config")

            else:        try:class DomoLogger:import traceback

                raise LogConfigError(f"Unknown handler type: {handler_type}")

            asyncio.get_running_loop()

    def _start_flush_task(self):

        """Start the background flush task"""            self._start_flush_task()    """Enhanced logger with structured logging and multiple handlers"""import os

        if self.flush_task is None:

            self.flush_task = asyncio.create_task(self._periodic_flush())        except RuntimeError:



    async def log(            # No event loop, task will be started when first log is calledimport inspect

        self,

        level: LogLevel,            pass

        message: str,

        **context    def __init__(self, config: LogConfig, app_name: str):import time

    ) -> bool:

        """Log a message with structured context"""    def _setup_handlers(self):



        # Check if we should log this level        """Setup handlers based on configuration"""        self.config = configimport socket

        if not self.config.level.should_log(level):

            return True        # Get handler configurations from the config



        # Start flush task if not already started and event loop is available        handler_configs = self.config.get_handler_configs()        self.app_name = app_namefrom abc import ABC, abstractmethod

        if self.flush_task is None:

            try:        

                asyncio.get_running_loop()

                self._start_flush_task()        for handler_config in handler_configs:        self.handlers: List[LogHandler] = []from contextvars import ContextVar

            except RuntimeError:

                pass            handler_type = handler_config["type"]



        # Create log entry            config = handler_config["config"]        self.buffer: List[LogEntry] = []from functools import wraps

        entry = LogEntry.create(

            level=level,            cloud_config = handler_config.get("cloud_config")

            message=message,

            logger=context.get('logger', f"domolibrary.{self.app_name}"),                    self.correlation_manager = correlation_managerfrom typing import Optional, List, Dict, Any, Union

            user=context.get('user'),

            action=context.get('action'),            if handler_type == "console":

            entity=context.get('entity'),

            status=context.get('status', 'info'),                self.handlers.append(ConsoleHandler(config))        self.flush_task = Nonefrom dataclasses import dataclass, field

            duration_ms=context.get('duration_ms'),

            trace_id=self.correlation_manager.trace_id_var.get(),            elif handler_type == "file":

            request_id=self.correlation_manager.request_id_var.get(),

            session_id=self.correlation_manager.session_id_var.get(),                self.handlers.append(FileHandler(config))

            correlation=self.correlation_manager.correlation_var.get(),

            multi_tenant=context.get('multi_tenant'),            elif handler_type == "cloud":

            http_details=context.get('http_details'),

            extra=context.get('extra', {}),                if cloud_config:        # Validate configurationfrom nbdev.showdoc import patch_to

        )

                    cloud_provider = cloud_config.get("cloud_provider")

        # Add to buffer

        self.buffer.append(entry)                    if cloud_provider == "datadog":        config.validate_config()s



        # Flush if buffer is full                        self.handlers.append(DatadogHandler(config))

        if len(self.buffer) >= self.config.batch_size:

            await self.flush()                    elif cloud_provider == "aws":



        return True                        self.handlers.append(AWSCloudWatchHandler(config))



    async def flush(self) -> bool:                    elif cloud_provider == "gcp":        # Initialize handlers based on config# No Datadog library imports needed - using direct HTTP API

        """Flush buffered entries to all handlers"""

        if not self.buffer:                        self.handlers.append(GCPLoggingHandler(config))

            return True

                    elif cloud_provider == "azure":        self._setup_handlers()

        entries_to_flush = self.buffer.copy()

        self.buffer.clear()                        self.handlers.append(AzureLogAnalyticsHandler(config))



        success = True                    else:

        for handler in self.handlers:

            if not await handler.write(entries_to_flush):                        raise LogConfigError(f"Unknown cloud provider: {cloud_provider}")

                success = False

                else:        # Try to start background flush task if event loop is available# Logging-specific exceptions

        return success

                    raise LogConfigError("Cloud handler missing cloud_config")

    async def _periodic_flush(self):

        """Background task to periodically flush logs"""            else:        try:class LoggingError(Exceptions):

        while True:

            await asyncio.sleep(self.config.flush_interval)                raise LogConfigError(f"Unknown handler type: {handler_type}")

            await self.flush()

            asyncio.get_running_loop()    """Base exception for logging errors"""

    # Convenience methods for different log levels

    async def debug(self, message: str, **context) -> bool:    def _start_flush_task(self):

        return await self.log(LogLevel.DEBUG, message, **context)

        """Start the background flush task"""            self._start_flush_task()    pass

    async def info(self, message: str, **context) -> bool:

        return await self.log(LogLevel.INFO, message, **context)        if self.flush_task is None:



    async def warning(self, message: str, **context) -> bool:            self.flush_task = asyncio.create_task(self._periodic_flush())        except RuntimeError:

        return await self.log(LogLevel.WARNING, message, **context)



    async def error(self, message: str, **context) -> bool:

        return await self.log(LogLevel.ERROR, message, **context)    async def log(            # No event loop, task will be started when first log is calledclass LogHandlerError(LoggingError):



    async def critical(self, message: str, **context) -> bool:        self,

        return await self.log(LogLevel.CRITICAL, message, **context)

        level: LogLevel,            pass    """Exception for log handler errors"""

    def start_request(self, parent_trace_id: Optional[str] = None, auth=None) -> str:

        """Start a new request context"""        message: str,

        return self.correlation_manager.start_request(parent_trace_id, auth)

        **context    pass

    def end_request(self):

        """End current request context"""    ) -> bool:

        # Clear context variables (they'll be reset on next request)

        pass        """Log a message with structured context"""    def _setup_handlers(self):



    async def close(self):

        """Clean up resources"""

        # Cancel flush task        # Check if we should log this level        """Setup handlers based on configuration"""class LogConfigError(LoggingError):

        if hasattr(self, 'flush_task') and self.flush_task:

            self.flush_task.cancel()        if not self.config.level.should_log(level):

            try:

                await self.flush_task            return True        # Get handler configurations from the config    """Exception for log configuration errors"""

            except asyncio.CancelledError:

                pass



        # Final flush        # Start flush task if not already started and event loop is available        handler_configs = self.config.get_handler_configs()    pass

        await self.flush()

        if self.flush_task is None:

        # Close handlers

        for handler in self.handlers:            try:        

            await handler.close()

                asyncio.get_running_loop()



# Global logger instance                self._start_flush_task()        for handler_config in handler_configs:class LogWriteError(LoggingError):

_global_logger: Optional[DomoLogger] = None

            except RuntimeError:



def get_logger(app_name: str = "domolibrary") -> DomoLogger:                pass            handler_type = handler_config["type"]    """Exception for log write errors"""

    """Get or create the global logger instance"""

    global _global_logger

    if _global_logger is None:

        config = AzureLogAnalyticsConfig.from_env()        # Create log entry            config = handler_config["config"]    pass

        _global_logger = DomoLogger(config, app_name)

    return _global_logger        entry = LogEntry.create(



            level=level,            cloud_config = handler_config.get("cloud_config")

def set_global_logger(logger: DomoLogger):

    """Set the global logger instance"""            message=message,

    global _global_logger

    _global_logger = logger            logger=context.get('logger', f"domolibrary.{self.app_name}"),            class LogFlushError(LoggingError):

            user=context.get('user'),

            action=context.get('action'),            if handler_type == "console":    """Exception for log flush errors"""

            entity=context.get('entity'),

            status=context.get('status', 'info'),                self.handlers.append(ConsoleHandler(config))    pass

            duration_ms=context.get('duration_ms'),

            trace_id=self.correlation_manager.trace_id_var.get(),            elif handler_type == "file":

            request_id=self.correlation_manager.request_id_var.get(),

            session_id=self.correlation_manager.session_id_var.get(),                self.handlers.append(FileHandler(config))

            correlation=self.correlation_manager.correlation_var.get(),

            multi_tenant=context.get('multi_tenant'),            elif handler_type == "cloud":class LogLevel(str, Enum):

            http_details=context.get('http_details'),

            extra=context.get('extra', {}),                if cloud_config:    """Standard logging levels"""

        )

                    cloud_provider = cloud_config.get("cloud_provider")    DEBUG = "DEBUG"

        # Add to buffer

        self.buffer.append(entry)                    if cloud_provider == "datadog":    INFO = "INFO"



        # Flush if buffer is full                        self.handlers.append(DatadogHandler(config))    WARNING = "WARNING"

        if len(self.buffer) >= self.config.batch_size:

            await self.flush()                    elif cloud_provider == "aws":    ERROR = "ERROR"



        return True                        self.handlers.append(AWSCloudWatchHandler(config))    CRITICAL = "CRITICAL"



    async def flush(self) -> bool:                    elif cloud_provider == "gcp":

        """Flush buffered entries to all handlers"""

        if not self.buffer:                        self.handlers.append(GCPLoggingHandler(config))    @classmethod

            return True

                    elif cloud_provider == "azure":    def from_string(cls, level_str: str) -> 'LogLevel':

        entries_to_flush = self.buffer.copy()

        self.buffer.clear()                        self.handlers.append(AzureLogAnalyticsHandler(config))        """Convert string to LogLevel enum"""



        success = True                    else:        try:

        for handler in self.handlers:

            if not await handler.write(entries_to_flush):                        raise LogConfigError(f"Unknown cloud provider: {cloud_provider}")            return cls(level_str.upper())

                success = False

                else:        except ValueError:

        return success

                    raise LogConfigError("Cloud handler missing cloud_config")            return cls.INFO  # default fallback

    async def _periodic_flush(self):

        """Background task to periodically flush logs"""            else:

        while True:

            await asyncio.sleep(self.config.flush_interval)                raise LogConfigError(f"Unknown handler type: {handler_type}")    def should_log(self, other: 'LogLevel') -> bool:

            await self.flush()

        """Check if this level should log the other level"""

    # Convenience methods for different log levels

    async def debug(self, message: str, **context) -> bool:    def _start_flush_task(self):        levels = list(LogLevel)

        return await self.log(LogLevel.DEBUG, message, **context)

        """Start the background flush task"""        return levels.index(self) <= levels.index(other)

    async def info(self, message: str, **context) -> bool:

        return await self.log(LogLevel.INFO, message, **context)        if self.flush_task is None:



    async def warning(self, message: str, **context) -> bool:            self.flush_task = asyncio.create_task(self._periodic_flush())

        return await self.log(LogLevel.WARNING, message, **context)

@dataclass

    async def error(self, message: str, **context) -> bool:

        return await self.log(LogLevel.ERROR, message, **context)    async def log(class Entity:



    async def critical(self, message: str, **context) -> bool:        self,    """Entity information for logging"""

        return await self.log(LogLevel.CRITICAL, message, **context)

        level: LogLevel,    type: str  # dataset, card, user, dataflow, page, etc.

    def start_request(self, parent_trace_id: Optional[str] = None, auth=None) -> str:

        """Start a new request context"""        message: str,    id: Optional[str] = None

        return self.correlation_manager.start_request(parent_trace_id, auth)

        **context    name: Optional[str] = None

    def end_request(self):

        """End current request context"""    ) -> bool:    additional_info: Dict[str, Any] = field(default_factory=dict)

        # Clear context variables (they'll be reset on next request)

        pass        """Log a message with structured context"""    parent : Any # instance of a class



    async def close(self):

        """Clean up resources"""

        # Cancel flush task        # Check if we should log this level    def get_additional_info(self, info_fn: Callable = None):

        if hasattr(self, 'flush_task') and self.flush_task:

            self.flush_task.cancel()        if not self.config.level.should_log(level):        if info_fn:

            try:

                await self.flush_task            return True            self.additional_info = info_fn(self)

            except asyncio.CancelledError:

                pass            return self.additional_info



        # Final flush        # Start flush task if not already started and event loop is available

        await self.flush()

        if self.flush_task is None:        additional_info = {}

        # Close handlers

        for handler in self.handlers:            try:        if hasattr(domo_entity, 'description'):

            await handler.close()

                asyncio.get_running_loop()            additional_info['description'] = getattr(domo_entity, 'description', '')



# Global logger instance                self._start_flush_task()        if hasattr(domo_entity, 'owner'):

_global_logger: Optional[DomoLogger] = None

            except RuntimeError:            additional_info['owner'] = getattr(domo_entity, 'owner', {})



def get_logger(app_name: str = "domolibrary") -> DomoLogger:                pass        if hasattr(domo_entity, 'display_type'):

    """Get or create the global logger instance"""

    global _global_logger            additional_info['display_type'] = getattr(domo_entity, 'display_type', '')

    if _global_logger is None:

        config = AzureLogAnalyticsConfig.from_env()        # Create log entry        if hasattr(domo_entity, 'data_provider_type'):

        _global_logger = DomoLogger(config, app_name)

    return _global_logger        entry = LogEntry.create(            additional_info['data_provider_type'] = getattr(domo_entity, 'data_provider_type', '')



            level=level,        

def set_global_logger(logger: DomoLogger):

    """Set the global logger instance"""            message=message,        # Get auth instance info

    global _global_logger

    _global_logger = logger            logger=context.get('logger', f"domolibrary.{self.app_name}"),        if hasattr(domo_entity, 'auth') and domo_entity.auth:

            user=context.get('user'),            additional_info['domo_instance'] = getattr(domo_entity.auth, 'domo_instance', None)

            action=context.get('action'),

            entity=context.get('entity'),        self.additional_info = additional_info

            status=context.get('status', 'info'),        return self.additional_info

            duration_ms=context.get('duration_ms'),

            trace_id=self.correlation_manager.trace_id_var.get(),

            request_id=self.correlation_manager.request_id_var.get(),

            session_id=self.correlation_manager.session_id_var.get(),    

            correlation=self.correlation_manager.correlation_var.get(),    @classmethod

            multi_tenant=context.get('multi_tenant'),    def from_domo_entity(cls, domo_entity, info_fn: Callable = None) -> 'Entity':

            http_details=context.get('http_details'),        """Create Entity from a DomoEntity object"""

            extra=context.get('extra', {}),        

        )        if not domo_entity:

            return None

        # Add to buffer    

        self.buffer.append(entry)        # Extract entity type from class name (e.g., DomoDataset -> dataset)

        entity_type = cls._extract_entity_type(type(domo_entity).__name__)

        # Flush if buffer is full    

        if len(self.buffer) >= self.config.batch_size:        entity = cls(

            await self.flush()            type=entity_type,

            parent = domo_entity

        return True            id=getattr(domo_entity, 'id', None),

            name=getattr(domo_entity, 'name', None),

    async def flush(self) -> bool:            additional_info=additional_info

        """Flush buffered entries to all handlers"""        )

        if not self.buffer:        entity.get_additional_info(info_fn = info_fn)

            return True

        return entity

        entries_to_flush = self.buffer.copy()    

        self.buffer.clear()    @staticmethod

    def _extract_entity_type(class_name: str) -> str:

        success = True        """Extract entity type from DomoEntity class name"""

        for handler in self.handlers:        # Remove 'Domo' prefix and convert to lowercase

            if not await handler.write(entries_to_flush):        if class_name.startswith('Domo'):

                success = False            return class_name[4:].lower()a

        return class_name.lower()

        return success



    async def _periodic_flush(self):@dataclass

        """Background task to periodically flush logs"""class HTTPDetails:

        while True:    """HTTP request/response details"""

            await asyncio.sleep(self.config.flush_interval)    method: Optional[str] = None

            await self.flush()    url: Optional[str] = None

    status_code: Optional[int] = None

    # Convenience methods for different log levels    headers: Optional[Dict[str, str]] = None

    async def debug(self, message: str, **context) -> bool:    params: Optional[Dict[str, Any]] = None

        return await self.log(LogLevel.DEBUG, message, **context)    response_size: Optional[int] = None

    request_body: Optional[Any] = None

    async def info(self, message: str, **context) -> bool:    response_body: Optional[Any] = None

        return await self.log(LogLevel.INFO, message, **context)



    async def warning(self, message: str, **context) -> bool:@dataclass

        return await self.log(LogLevel.WARNING, message, **context)class Correlation:

    """Correlation information for distributed tracing"""

    async def error(self, message: str, **context) -> bool:    trace_id: Optional[str] = None

        return await self.log(LogLevel.ERROR, message, **context)    span_id: Optional[str] = None

    parent_span_id: Optional[str] = None

    async def critical(self, message: str, **context) -> bool:

        return await self.log(LogLevel.CRITICAL, message, **context)

@dataclass

    def start_request(self, parent_trace_id: Optional[str] = None, auth=None) -> str:class MultiTenant:

        """Start a new request context"""    """Multi-tenant information"""

        return self.correlation_manager.start_request(parent_trace_id, auth)    user_id: Optional[str] = None

    session_id: Optional[str] = None

    def end_request(self):    tenant_id: Optional[str] = None

        """End current request context"""    organization_id: Optional[str] = None

        # Clear context variables (they'll be reset on next request)

        pass

@dataclass

    async def close(self):class LogEntry:

        """Clean up resources"""    """Enhanced log entry with structured JSON format"""

        # Cancel flush task

        if hasattr(self, 'flush_task') and self.flush_task:    # Core log fields

            self.flush_task.cancel()    timestamp: str

            try:    level: LogLevel

                await self.flush_task    logger: str

            except asyncio.CancelledError:    message: str

                pass

    # Business context

        # Final flush    user: Optional[str] = None

        await self.flush()    action: Optional[str] = None

    entity: Optional[Entity] = None

        # Close handlers    status: str = "info"

        for handler in self.handlers:    duration_ms: Optional[int] = None

            await handler.close()

    # Distributed tracing

    correlation: Optional[Correlation] = None

# Global logger instance

_global_logger: Optional[DomoLogger] = None    # Multi-tenant context

    multi_tenant: Optional[MultiTenant] = None



def get_logger(app_name: str = "domolibrary") -> DomoLogger:    # HTTP details (for API calls)

    """Get or create the global logger instance"""    http_details: Optional[HTTPDetails] = None

    global _global_logger

    if _global_logger is None:    # Flexible metadata

        config = AzureLogAnalyticsConfig.from_env()    extra: Dict[str, Any] = field(default_factory=dict)

        _global_logger = DomoLogger(config, app_name)

    return _global_logger    def to_dict(self) -> Dict[str, Any]:

        """Convert to dictionary for JSON serialization"""

        result = {

def set_global_logger(logger: DomoLogger):            # Core log fields

    """Set the global logger instance"""            "timestamp": self.timestamp,

    global _global_logger            "level": self.level.value,

    _global_logger = logger            "logger": self.logger,
            "message": self.message,

            # Business context
            "user": self.user or (self.multi_tenant.user_id if self.multi_tenant and self.multi_tenant.user_id else None),
            "action": self.action,
            "status": self.status,
            "duration_ms": self.duration_ms,

            # Entity (serialize if present)
            "entity": self.entity.__dict__ if self.entity else None,

            # Correlation (serialize if present)
            "correlation": self.correlation.__dict__ if self.correlation else None,

            # Multi-tenant (serialize if present)
            "multi_tenant": self.multi_tenant.__dict__ if self.multi_tenant else None,

            # HTTP details (serialize if present and has data)
            "http_details": self._serialize_http_details(),

            # Flexible metadata
            "extra": self.extra,
        }

        # Remove None values for cleaner output
        return {k: v for k, v in result.items() if v is not None}

    def to_json(self) -> str:
        """Convert to JSON string"""
        return json.dumps(self.to_dict(), default=str)

    @classmethod
    def create(
        cls,
        level: LogLevel,
        message: str,
        logger: str,
        **kwargs
    ) -> 'LogEntry':
        # Debug: Print what we receive for multi_tenant and http_details
        # if 'get_data' in message:
        #     multi_tenant = kwargs.get('multi_tenant')
        #     http_details = kwargs.get('http_details')
        #     print(f" LogEntry.create() multi_tenant={multi_tenant}, http_details={http_details is not None}")
        #     if multi_tenant:
        #         print(f" multi_tenant type: {type(multi_tenant)}, content: {multi_tenant}")
        #     if http_details:
        #         print(f" http_details type: {type(http_details)}")
        """Factory method to create a LogEntry with current timestamp"""
        timestamp = dt.datetime.utcnow().isoformat() + 'Z'

        # Extract known fields
        user = kwargs.get('user')
        action = kwargs.get('action')
        status = kwargs.get('status', 'info')
        duration_ms = kwargs.get('duration_ms')
        extra = kwargs.get('extra', {})

        # Handle entity - could be dict, Entity object, or DomoEntity object
        entity = kwargs.get('entity')
        if isinstance(entity, dict) and entity:
            entity_obj = Entity(**entity)
        elif isinstance(entity, Entity):
            entity_obj = entity
        elif entity and hasattr(entity, 'id'):  # DomoEntity object
            entity_obj = Entity.from_domo_entity(entity)
        else:
            entity_obj = None

        # Handle correlation - could be dict, Correlation object, or individual fields
        correlation_obj = None
        correlation = kwargs.get('correlation')
        if isinstance(correlation, dict) and correlation:
            correlation_obj = Correlation(**correlation)
        elif isinstance(correlation, Correlation):
            correlation_obj = correlation
        elif any(k in kwargs for k in ['trace_id', 'span_id', 'parent_span_id']):
            # Create correlation from individual fields
            correlation_obj = Correlation(
                trace_id=kwargs.get('trace_id'),
                span_id=kwargs.get('span_id'),
                parent_span_id=kwargs.get('parent_span_id')
            )

        # Handle multi-tenant - could be dict, MultiTenant object, or individual fields
        multi_tenant_obj = None
        multi_tenant = kwargs.get('multi_tenant')
        if isinstance(multi_tenant, dict) and multi_tenant:
            multi_tenant_obj = MultiTenant(**multi_tenant)
        elif isinstance(multi_tenant, MultiTenant):
            multi_tenant_obj = multi_tenant
        elif any(k in kwargs for k in ['user_id', 'session_id', 'tenant_id', 'organization_id']):
            # Create multi-tenant from individual fields
            multi_tenant_obj = MultiTenant(
                user_id=kwargs.get('user_id') or kwargs.get('user'),
                session_id=kwargs.get('session_id'),
                tenant_id=kwargs.get('tenant_id'),
                organization_id=kwargs.get('organization_id')
            )

        # Handle HTTP details - could be dict, HTTPDetails object, or individual fields
        http_details_obj = None
        http_details = kwargs.get('http_details')
        if isinstance(http_details, dict) and http_details:
            http_details_obj = HTTPDetails(**http_details)
        elif isinstance(http_details, HTTPDetails):
            http_details_obj = http_details
        elif any(k in kwargs for k in ['method', 'url', 'status_code', 'headers', 'response_size']):
            # Create HTTP details from individual fields
            http_details_obj = HTTPDetails(
                method=kwargs.get('method'),
                url=kwargs.get('url'),
                status_code=kwargs.get('status_code'),
                headers=kwargs.get('headers'),
                response_size=kwargs.get('response_size'),
                request_body=kwargs.get('request_body'),
                response_body=kwargs.get('response_body')
            )

        # If user is not set but multi_tenant has user_id, use that
        if not user and multi_tenant_obj and multi_tenant_obj.user_id:
            user = multi_tenant_obj.user_id

        return cls(
            timestamp=timestamp,
            level=level,
            logger=logger,
            message=message,
            user=user,
            action=action,
            entity=entity_obj,
            status=status,
            duration_ms=duration_ms,
            correlation=correlation_obj,
            multi_tenant=multi_tenant_obj,
            http_details=http_details_obj,
            extra=extra
        )

    def _serialize_http_details(self) -> Optional[Dict[str, Any]]:
        """Serialize HTTP details for logging, filtering sensitive data"""
        if not self.http_details:
            return None

        http_details_dict = {}

        if self.http_details.method:
            http_details_dict["method"] = self.http_details.method

        if self.http_details.url:
            http_details_dict["url"] = self.http_details.url

        if self.http_details.headers:
            # Only include important headers, not sensitive ones
            safe_headers = {}
            for k, v in self.http_details.headers.items():
                if k.lower() not in ['authorization', 'cookie', 'x-domo-authentication']:
                    safe_headers[k] = v
            if safe_headers:
                http_details_dict["headers"] = safe_headers

        if self.http_details.params:
            http_details_dict["params"] = self.http_details.params

        if self.http_details.request_body:
            # Truncate large request bodies
            if isinstance(self.http_details.request_body, str) and len(self.http_details.request_body) > 500:
                http_details_dict["request_body"] = self.http_details.request_body[:500] + "..."
            else:
                http_details_dict["request_body"] = self.http_details.request_body

        if self.http_details.response_body:
            # Truncate large response bodies
            if isinstance(self.http_details.response_body, str) and len(self.http_details.response_body) > 500:
                http_details_dict["response_body"] = self.http_details.response_body[:500] + "..."
            else:
                http_details_dict["response_body"] = self.http_details.response_body

        return http_details_dict if http_details_dict else None


class LogConfig(ABC):
    """Abstract base configuration for logging system"""

    def __init__(
        self,
        level: LogLevel = LogLevel.INFO,
        output_mode: str = "console",  # console, file, eventbus, cloud
        format: str = "json",  # json, text
        destination: Optional[str] = None,  # file path, webhook URL, etc.
        batch_size: int = 100,
        flush_interval: int = 30,  # seconds
        correlation_enabled: bool = True,
        include_traceback: bool = True,
        max_buffer_size: int = 1000,
        pretty_print: bool = False,  # Pretty print JSON for development
    ):
        self.level = level
        self.output_mode = output_mode
        self.format = format
        self.destination = destination
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.correlation_enabled = correlation_enabled
        self.include_traceback = include_traceback
        self.max_buffer_size = max_buffer_size
        self.pretty_print = pretty_print

    @abstractmethod
    def get_cloud_config(self) -> Dict[str, Any]:
        """Get cloud provider specific configuration"""
        pass

    @abstractmethod
    def validate_config(self) -> bool:
        """Validate the configuration"""
        pass

    def get_handler_configs(self) -> List[Dict[str, Any]]:
        """Get handler configurations for this config. Default implementation returns single handler."""
        return [{
            "type": self.output_mode,
            "config": self,
            "cloud_config": self.get_cloud_config() if self.output_mode == "cloud" else None
        }]


    @classmethod
    create_file_config():
        console_config = ConsoleLogConfig(
        level=level,
        pretty_print=pretty_print,
        **kwargs
    )
    file_config = ConsoleLogConfig(
        level=level,
        output_mode="file",
        destination=file_path,
        pretty_print=False,  # File doesn't need pretty printing
        **kwargs
    )


@dataclass
class HandlerConfig:
    type : str
    config : LogConfig

@dataclass
class MultiHandlerLogConfig(LogConfig):
    """Configuration that supports multiple handlers simultaneously"""

    handlers : List[HandlerConfig]
    output_mode: str =  "multi",
    level: LogLevel = LogLevel.INFO,
    batch_size: int = 100,
    flush_interval: int = 30,
    correlation_enabled: bool = True,
    include_traceback: bool = True,
    max_buffer_size: int = 1000,

    def get_cloud_config(self) -> Dict[str, Any]:
        """Return empty config since this handles multiple providers"""
        return {"cloud_provider": "multi"}

    def get_handler_configs(self) -> List[Dict[str, Any]]:
        """Return all handler configurations"""
        return self.handlers

    # Convenience functions for creating multi-handler configurations
    @classmethod
    def create(
        handlers: List[Dict[str, Any]],
        level: LogLevel = LogLevel.INFO,
        batch_size: int = 100,
        flush_interval: int = 30,
        **kwargs
    ) -> MultiHandlerLogConfig:
        """Create a multi-handler configuration with custom handlers"""
        return MultiHandlerLogConfig(
            handlers=handlers,
            level=level,
            batch_size=batch_size,
            flush_interval=flush_interval,
            **kwargs
        )



@classmethodf
def create_console_file_config(
    file_path: str,
    level: LogLevel = LogLevel.INFO,
    pretty_print: bool = True,
    **kwargs
) -> MultiHandlerLogConfig:
    """Create a configuration that logs to both console and file"""
    
    
    return MultiHandlerLogConfig(
        handlers=[
            {"type": "console", "config": console_config},
            {"type": "file", "config": file_config}
        ],
        level=level,
        **kwargs
    )


def create_console_datadog_config(
    datadog_api_key: Optional[str] = None,
    datadog_app_key: Optional[str] = None,
    datadog_site: str = "datadoghq.com",
    datadog_service: str = "domolibrary",
    datadog_env: str = "production",
    level: LogLevel = LogLevel.INFO,
    pretty_print: bool = True,
    **kwargs
) -> MultiHandlerLogConfig:
    """Create a configuration that logs to both console and Datadog"""
    console_config = ConsoleLogConfig(
        level=level,
        pretty_print=pretty_print,
        **kwargs
    )
    datadog_config = DatadogLogConfig(
        api_key=datadog_api_key,
        app_key=datadog_app_key,
        site=datadog_site,
        service=datadog_service,
        env=datadog_env,
        level=level,
        **kwargs
    )
    
    return MultiHandlerLogConfig(
        handlers=[
            {"type": "console", "config": console_config},
            {"type": "cloud", "config": datadog_config, "cloud_config": datadog_config.get_cloud_config()}
        ],
        level=level,
        **kwargs
    )


def create_console_file_datadog_config(
    file_path: str,
    datadog_api_key: Optional[str] = None,
    datadog_app_key: Optional[str] = None,
    datadog_site: str = "datadoghq.com",
    datadog_service: str = "domolibrary",
    datadog_env: str = "production",
    level: LogLevel = LogLevel.INFO,
    pretty_print: bool = True,
    **kwargs
) -> MultiHandlerLogConfig:
    """Create a configuration that logs to console, file, and Datadog"""
    console_config = ConsoleLogConfig(
        level=level,
        pretty_print=pretty_print,
        **kwargs
    )
    file_config = ConsoleLogConfig(
        level=level,
        output_mode="file",
        destination=file_path,
        pretty_print=False,  
        **kwargs
    )
    datadog_config = DatadogLogConfig(
        api_key=datadog_api_key,
        app_key=datadog_app_key,
        site=datadog_site,
        service=datadog_service,
        env=datadog_env,
        level=level,
        **kwargs
    )
    
    return MultiHandlerLogConfig(
        handlers=[
            {"type": "console", "config": console_config},
            {"type": "file", "config": file_config},
            {"type": "cloud", "config": datadog_config, "cloud_config": datadog_config.get_cloud_config()}
        ],
        level=level,
        **kwargs
    )

def create_file_datadog_config(
    file_path: str,
    datadog_api_key: Optional[str] = None,
    datadog_app_key: Optional[str] = None,
    datadog_site: str = "datadoghq.com",
    datadog_service: str = "domolibrary",
    datadog_env: str = "production",
    level: LogLevel = LogLevel.INFO,
    pretty_print: bool = True,
    **kwargs
) -> MultiHandlerLogConfig:
    """Create a configuration that logs to file and Datadog"""
    file_config = ConsoleLogConfig(
        level=level,
        output_mode="file",
        destination=file_path,
        pretty_print=False,  
        **kwargs
    )
    datadog_config = DatadogLogConfig(
        api_key=datadog_api_key,
        app_key=datadog_app_key,
        site=datadog_site,
        service=datadog_service,
        env=datadog_env,
        level=level,
        **kwargs
    )
    return MultiHandlerLogConfig(
        handlers=[
            {"type": "file", "config": file_config},
            {"type": "cloud", "config": datadog_config, "cloud_config": datadog_config.get_cloud_config()}
        ],
        level=level,
        **kwargs
    )

class DatadogLogConfig(LogConfig):
    """Datadog-specific log configuration"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        app_key: Optional[str] = None,
        site: str = "datadoghq.com",
        service: str = "domolibrary",
        env: str = "production",
        **kwargs
    ):
        super().__init__(**kwargs)
        self.api_key = api_key or os.getenv('DATADOG_API_KEY')
        self.app_key = app_key or os.getenv('DATADOG_APP_KEY')
        self.site = site
        self.service = service
        self.env = env
        self.output_mode = "cloud"
        self.cloud_provider = "datadog"

    def get_cloud_config(self) -> Dict[str, Any]:
        return {
            "api_key": self.api_key,
            "app_key": self.app_key,
            "site": self.site,
            "service": self.service,
            "env": self.env,
            "cloud_provider": self.cloud_provider
        }

    def validate_config(self) -> bool:
        if not self.api_key:
            raise LogConfigError("Datadog API key is required")
        return True


class AWSCloudWatchLogConfig(LogConfig):
    """AWS CloudWatch-specific log configuration"""

    def __init__(
        self,
        aws_region: Optional[str] = None,
        log_group: Optional[str] = None,
        log_stream: Optional[str] = None,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.aws_region = aws_region or os.getenv('AWS_REGION', 'us-east-1')
        self.log_group = log_group or os.getenv('AWS_LOG_GROUP', 'domolibrary')
        self.log_stream = log_stream or os.getenv('AWS_LOG_STREAM')
        self.output_mode = "cloud"
        self.cloud_provider = "aws"

    def get_cloud_config(self) -> Dict[str, Any]:
        return {
            "aws_region": self.aws_region,
            "log_group": self.log_group,
            "log_stream": self.log_stream,
            "cloud_provider": self.cloud_provider
        }

    def validate_config(self) -> bool:
        if not self.aws_region:
            raise LogConfigError("AWS region is required")
        if not self.log_group:
            raise LogConfigError("AWS log group is required")
        return True


class GCPLoggingConfig(LogConfig):
    """Google Cloud Logging-specific configuration"""

    def __init__(
        self,
        project_id: Optional[str] = None,
        log_name: Optional[str] = None,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.project_id = project_id or os.getenv('GCP_PROJECT')
        self.log_name = log_name or os.getenv('GCP_LOG_NAME', 'domolibrary')
        self.output_mode = "cloud"
        self.cloud_provider = "gcp"

    def get_cloud_config(self) -> Dict[str, Any]:
        return {
            "project_id": self.project_id,
            "log_name": self.log_name,
            "cloud_provider": self.cloud_provider
        }

    def validate_config(self) -> bool:
        if not self.project_id:
            raise LogConfigError("GCP project ID is required")
        return True


class AzureLogAnalyticsConfig(LogConfig):
    """Azure Log Analytics-specific configuration"""

    def __init__(
        self,
        workspace_id: Optional[str] = None,
        shared_key: Optional[str] = None,
        log_type: str = "domolibrary",
        **kwargs
    ):
        super().__init__(**kwargs)
        self.workspace_id = workspace_id or os.getenv('AZURE_WORKSPACE_ID')
        self.shared_key = shared_key or os.getenv('AZURE_SHARED_KEY')
        self.log_type = log_type
        self.output_mode = "cloud"
        self.cloud_provider = "azure"

    def get_cloud_config(self) -> Dict[str, Any]:
        return {
            "workspace_id": self.workspace_id,
            "shared_key": self.shared_key,
            "log_type": self.log_type,
            "cloud_provider": self.cloud_provider
        }

    def validate_config(self) -> bool:
        if not self.workspace_id:
            raise LogConfigError("Azure workspace ID is required")
        if not self.shared_key:
            raise LogConfigError("Azure shared key is required")
        return True

    @classmethod
    def from_env(cls) -> 'LogConfig':
        """Create config from environment variables"""
        cloud_provider = os.getenv('LOG_CLOUD_PROVIDER', 'console')
        
        if cloud_provider == 'datadog':
            return DatadogLogConfig(
                level=LogLevel.from_string(os.getenv('LOG_LEVEL', 'INFO')),
                format=os.getenv('LOG_FORMAT', 'json'),
                batch_size=int(os.getenv('LOG_BATCH_SIZE', '100')),
                flush_interval=int(os.getenv('LOG_FLUSH_INTERVAL', '30')),
                correlation_enabled=os.getenv('LOG_CORRELATION_ENABLED', 'true').lower() == 'true',
                include_traceback=os.getenv('LOG_INCLUDE_TRACEBACK', 'true').lower() == 'true',
                max_buffer_size=int(os.getenv('LOG_MAX_BUFFER_SIZE', '1000')),
                pretty_print=os.getenv('LOG_PRETTY_PRINT', 'false').lower() == 'true',
            )
        elif cloud_provider == 'aws':
            return AWSCloudWatchLogConfig(
                level=LogLevel.from_string(os.getenv('LOG_LEVEL', 'INFO')),
                format=os.getenv('LOG_FORMAT', 'json'),
                batch_size=int(os.getenv('LOG_BATCH_SIZE', '100')),
                flush_interval=int(os.getenv('LOG_FLUSH_INTERVAL', '30')),
                correlation_enabled=os.getenv('LOG_CORRELATION_ENABLED', 'true').lower() == 'true',
                include_traceback=os.getenv('LOG_INCLUDE_TRACEBACK', 'true').lower() == 'true',
                max_buffer_size=int(os.getenv('LOG_MAX_BUFFER_SIZE', '1000')),
                pretty_print=os.getenv('LOG_PRETTY_PRINT', 'false').lower() == 'true',
            )
        elif cloud_provider == 'gcp':
            return GCPLoggingConfig(
                level=LogLevel.from_string(os.getenv('LOG_LEVEL', 'INFO')),
                format=os.getenv('LOG_FORMAT', 'json'),
                batch_size=int(os.getenv('LOG_BATCH_SIZE', '100')),
                flush_interval=int(os.getenv('LOG_FLUSH_INTERVAL', '30')),
                correlation_enabled=os.getenv('LOG_CORRELATION_ENABLED', 'true').lower() == 'true',
                include_traceback=os.getenv('LOG_INCLUDE_TRACEBACK', 'true').lower() == 'true',
                max_buffer_size=int(os.getenv('LOG_MAX_BUFFER_SIZE', '1000')),
                pretty_print=os.getenv('LOG_PRETTY_PRINT', 'false').lower() == 'true',
            )
        elif cloud_provider == 'azure':
            return AzureLogAnalyticsConfig(
                level=LogLevel.from_string(os.getenv('LOG_LEVEL', 'INFO')),
                format=os.getenv('LOG_FORMAT', 'json'),
                batch_size=int(os.getenv('LOG_BATCH_SIZE', '100')),
                flush_interval=int(os.getenv('LOG_FLUSH_INTERVAL', '30')),
                correlation_enabled=os.getenv('LOG_CORRELATION_ENABLED', 'true').lower() == 'true',
                include_traceback=os.getenv('LOG_INCLUDE_TRACEBACK', 'true').lower() == 'true',
                max_buffer_size=int(os.getenv('LOG_MAX_BUFFER_SIZE', '1000')),
                pretty_print=os.getenv('LOG_PRETTY_PRINT', 'false').lower() == 'true',
            )
        else:
            # Default console configuration
            return ConsoleLogConfig(
                level=LogLevel.from_string(os.getenv('LOG_LEVEL', 'INFO')),
                format=os.getenv('LOG_FORMAT', 'json'),
                destination=os.getenv('LOG_DESTINATION'),
                batch_size=int(os.getenv('LOG_BATCH_SIZE', '100')),
                flush_interval=int(os.getenv('LOG_FLUSH_INTERVAL', '30')),
                correlation_enabled=os.getenv('LOG_CORRELATION_ENABLED', 'true').lower() == 'true',
                include_traceback=os.getenv('LOG_INCLUDE_TRACEBACK', 'true').lower() == 'true',
                max_buffer_size=int(os.getenv('LOG_MAX_BUFFER_SIZE', '1000')),
                pretty_print=os.getenv('LOG_PRETTY_PRINT', 'false').lower() == 'true',
            )


class ConsoleLogConfig(LogConfig):
    """Console-specific log configuration"""
    
    def get_cloud_config(self) -> Dict[str, Any]:
        return {"cloud_provider": "console"}
    
    def validate_config(self) -> bool:
        return True


class CorrelationManager:
    """Manages correlation IDs and context propagation"""

    def __init__(self):
        self.trace_id_var: ContextVar[Optional[str]] = ContextVar('trace_id', default=None)
        self.request_id_var: ContextVar[Optional[str]] = ContextVar('request_id', default=None)
        self.session_id_var: ContextVar[Optional[str]] = ContextVar('session_id', default=None)
        self.span_id_var: ContextVar[Optional[str]] = ContextVar('span_id', default=None)
        self.correlation_var: ContextVar[Optional[Correlation]] = ContextVar('correlation', default=None)
        # Track last span_id per trace_id for proper parent span relationships
        self._trace_span_history: Dict[str, str] = {}

    def generate_trace_id(self) -> str:
        """Generate a new trace ID"""
        return str(uuid.uuid4())

    def generate_request_id(self) -> str:
        """Generate a new request ID"""
        return uuid.uuid4().hex[:12]

    def generate_span_id(self) -> str:
        """Generate a new span ID"""
        return uuid.uuid4().hex[:16]

    def generate_session_id(self, auth=None) -> str:
        """Generate a new session ID based on auth or create random"""
        if auth:
            # Use auth instance and user info for session ID
            user_id = getattr(auth, 'user_id', None) or getattr(auth, 'user_name', None) or getattr(auth, 'username', None)
            domo_instance = getattr(auth, 'domo_instance', None)

            if domo_instance and user_id:
                return f"{domo_instance}_{user_id}"
            elif domo_instance:
                return f"{domo_instance}_anonymous"
            elif user_id:
                return f"unknown_{user_id}"
            else:
                return f"auth_{id(auth)}"
        else:
            return uuid.uuid4().hex[:12]

    def start_request(self, parent_trace_id: Optional[str] = None, auth=None, is_pagination_request: bool = False) -> str:
        """Start a new request context"""
        # Use existing trace_id if available, otherwise generate new one
        # Only generate new trace_id if we don't have one in context AND no parent provided
        current_trace_id = self.trace_id_var.get()
        trace_id = parent_trace_id or current_trace_id or self.generate_trace_id()
        
        request_id = self.generate_request_id()
        
        # Generate session_id from auth if available, otherwise use existing or generate random
        if auth and (hasattr(auth, 'user_id') or hasattr(auth, 'domo_instance')):
            session_id = self.generate_session_id(auth)
        else:
            session_id = self.session_id_var.get() or self.generate_session_id(auth)
        span_id = self.generate_span_id()

        # Handle parent span for pagination vs regular requests
        if is_pagination_request:
            # For pagination requests, use the original parent span for this trace
            # This ensures all pagination requests have the same parent
            parent_span_id = self._trace_span_history.get(f"{trace_id}_original_parent")
            if not parent_span_id:
                # If no original parent stored, this is the first pagination request
                # Store current span as original parent for future pagination requests
                parent_span_id = self._trace_span_history.get(trace_id)
                self._trace_span_history[f"{trace_id}_original_parent"] = parent_span_id or None
        else:
            # For regular requests, use normal span chaining
            parent_span_id = self._trace_span_history.get(trace_id)
            # Store this as the original parent for future pagination requests
            self._trace_span_history[f"{trace_id}_original_parent"] = parent_span_id

        # Update the span history with the current span_id for this trace
        self._trace_span_history[trace_id] = span_id

        # Set context variables
        self.trace_id_var.set(trace_id)
        self.request_id_var.set(request_id)
        self.session_id_var.set(session_id)
        self.span_id_var.set(span_id)

        # Create correlation object
        correlation = Correlation(
            trace_id=trace_id,
            span_id=span_id,
            parent_span_id=parent_span_id
        )
        self.correlation_var.set(correlation)

        return request_id

    def get_current_context(self) -> Dict[str, Any]:
        """Get current correlation context"""
        correlation = self.correlation_var.get()
        return {
            'trace_id': self.trace_id_var.get(),
            'request_id': self.request_id_var.get(),
            'session_id': self.session_id_var.get(),
            'span_id': self.span_id_var.get(),
            'correlation': correlation.__dict__ if correlation else None,
        }

    def set_context_value(self, key: str, value: Any):
        """Set a value in the correlation context"""
        correlation = self.correlation_var.get().copy()
        correlation[key] = value
        self.correlation_var.set(correlation)

# Global correlation manager instance
correlation_manager = CorrelationManager()


def extract_entity_from_args(args, kwargs) -> Optional[Entity]:
    """Extract entity information from function arguments"""
    # First, check if any argument is a DomoEntity object
    for arg in args:
        if hasattr(arg, 'id') and hasattr(arg, 'auth'):
            # This looks like a DomoEntity
            return Entity.from_domo_entity(arg)
    
    # Check for entity ID parameters in kwargs
    entity_id = None
    entity_type = None
    
    # Common entity ID parameter patterns
    for param_name in ['dataset_id', 'user_id', 'group_id', 'card_id', 'page_id', 'dataflow_id']:
        if param_name in kwargs:
            entity_id = kwargs[param_name]
            entity_type = param_name.replace('_id', '')
            break
    
    # Try to extract from URL if available
    url = kwargs.get('url', '')
    if not entity_type and url:
        import re
        if '/datasets/' in url:
            dataset_match = re.search(r'/datasets/([a-zA-Z0-9\-]+)', url)
            if dataset_match:
                entity_id = dataset_match.group(1)
                entity_type = 'dataset'
        elif '/cards/' in url:
            card_match = re.search(r'/cards/([a-zA-Z0-9\-]+)', url)
            if card_match:
                entity_id = card_match.group(1)
                entity_type = 'card'
        elif '/users/' in url:
            user_match = re.search(r'/users/([a-zA-Z0-9\-]+)', url)
            if user_match:
                entity_id = user_match.group(1)
                entity_type = 'user'
        elif '/pages/' in url:
            page_match = re.search(r'/pages/([a-zA-Z0-9\-]+)', url)
            if page_match:
                entity_id = page_match.group(1)
                entity_type = 'page'
    
    if entity_type and entity_id:
        # Try to get entity name from self object if available
        entity_name = None
        if args and hasattr(args[0], 'name'):
            entity_name = getattr(args[0], 'name', None)
        
        return Entity(
            type=entity_type,
            id=entity_id,
            name=entity_name
        )
    
    return None


class LogHandler(ABC):
    """Base class for log handlers"""

    def __init__(self, config: LogConfig):
        self.config = config

    @abstractmethod
    async def write(self, entries: List[LogEntry]) -> bool:
        """Write log entries to destination"""
        pass

    @abstractmethod
    async def flush(self) -> bool:
        """Flush any buffered entries"""
        pass

    async def close(self):
        """Clean up resources"""
        pass


class ConsoleHandler(LogHandler):
    """Handler for console output"""

    async def write(self, entries: List[LogEntry]) -> bool:
        """Write entries to console"""
        try:
            for entry in entries:
                if self.config.format == "json":
                    if self.config.pretty_print:
                        # Pretty print JSON for development
                        print(json.dumps(entry.to_dict(), indent=2, default=str))
                        print("-" * 80)  # Separator for readability
                    else:
                        print(entry.to_json())
                else:
                    print(f"[{entry.timestamp}] {entry.level.value} {entry.logger}: {entry.message}")
            return True
        except Exception as e:
            print(f"Error writing to console: {e}")
            return False

    async def flush(self) -> bool:
        """Console output doesn't need flushing"""
        return True


class FileHandler(LogHandler):
    """Handler for file output"""

    def __init__(self, config: LogConfig):
        super().__init__(config)
        if not config.destination:
            raise LogConfigError("File destination is required for FileHandler")
        self.file_path = config.destination
        
        # Only create directory if the file path has a directory component
        file_dir = os.path.dirname(self.file_path)
        if file_dir:  # Only create directory if there's a directory path
            try:
                os.makedirs(file_dir, exist_ok=True)
            except PermissionError as e:
                raise LogHandlerError(f"Permission denied creating directory for {self.file_path}: {e}")
            except OSError as e:
                raise LogHandlerError(f"OS error creating directory for {self.file_path}: {e}")

    async def write(self, entries: List[LogEntry]) -> bool:
        """Write entries to file"""
        try:
            with open(self.file_path, 'a', encoding='utf-8') as f:
                for entry in entries:
                    if self.config.format == "json":
                        f.write(entry.to_json() + '\n')
                    else:
                        f.write(f"[{entry.timestamp}] {entry.level.value} {entry.logger}: {entry.message}\n")
            return True
        except PermissionError as e:
            raise LogWriteError(f"Permission denied writing to file {self.file_path}: {e}")
        except OSError as e:
            raise LogWriteError(f"OS error writing to file {self.file_path}: {e}")
        except IOError as e:
            raise LogWriteError(f"IO error writing to file {self.file_path}: {e}")
        except Exception as e:
            raise LogWriteError(f"Unexpected error writing to file {self.file_path}: {e}")

    async def flush(self) -> bool:
        """File writes are synchronous, no flush needed"""
        return True


class CloudHandler(LogHandler):
    """Base class for cloud log handlers"""
    
    def __init__(self, config: LogConfig):
        super().__init__(config)
        self.cloud_config = config.get_cloud_config()
        config.validate_config()

    @abstractmethod
    async def _send_to_cloud(self, entries: List[LogEntry]) -> bool:
        """Send entries to cloud provider"""
        pass

    async def write(self, entries: List[LogEntry]) -> bool:
        """Write entries to cloud provider"""
        try:
            return await self._send_to_cloud(entries)
        except Exception as e:
            raise LogWriteError(f"Error sending logs to cloud provider: {e}")

    async def flush(self) -> bool:
        """Cloud handlers may need batching, implement in subclasses"""
        return True


class DatadogHandler(CloudHandler):
    """Datadog log handler using direct HTTP API"""

    def __init__(self, config: LogConfig):
        super().__init__(config)
        self._validate_config()

    def _validate_config(self):
        """Validate Datadog configuration"""
        api_key = self.cloud_config.get('api_key')
        if not api_key:
            raise LogHandlerError("Datadog API key is required")

    def _get_hostname(self) -> str:
        """Get the actual hostname/IP address of the machine"""
        try:
            # Try to get the hostname, fallback to IP if needed
            hostname = socket.gethostname()
            # Get the IP address for more specific identification
            ip_address = socket.gethostbyname(hostname)
            return ip_address
        except:
            # Fallback to localhost if hostname resolution fails
            return "127.0.0.1"

    def _convert_log_level(self, level: LogLevel) -> str:
        """Convert LogLevel enum to Datadog log level"""
        level_mapping = {
            LogLevel.DEBUG: "debug",
            LogLevel.INFO: "info",
            LogLevel.WARNING: "warning",
            LogLevel.ERROR: "error",
            LogLevel.CRITICAL: "critical"
        }
        return level_mapping.get(level, "info")


    def _send_logs_simple_api(self, entries: List[LogEntry]) -> bool:
        """Send logs using direct HTTP requests to Datadog"""
        try:
            import requests

            # Get configuration
            api_key = self.cloud_config.get('api_key')
            site = self.cloud_config.get('site', 'datadoghq.com')

            # Determine the intake URL based on site
            if site == 'datadoghq.com':
                intake_url = 'https://http-intake.logs.datadoghq.com/v1/input'
            elif site.startswith('us'):
                region = site.replace('.datadoghq.com', '')
                intake_url = f'https://http-intake.logs.{region}.datadoghq.com/v1/input'
            else:
                intake_url = f'https://http-intake.logs.{site}/v1/input'

            # Convert entries to log format
            logs_data = []
            hostname = self._get_hostname()
            for entry in entries:
                log_data = {
                    "message": entry.message,
                    "ddsource": "domolibrary",
                    "service": self.cloud_config.get('service', 'domolibrary'),
                    "hostname": hostname,
                    "status": self._convert_log_level(entry.level),
                    "ddtags": f"env:{self.cloud_config.get('env', 'production')},service:{self.cloud_config.get('service', 'domolibrary')}",
                    "timestamp": entry.timestamp
                }

                # Add structured data
                if entry.entity:
                    log_data["entity"] = entry.entity.__dict__

                if entry.correlation:
                    log_data["correlation"] = {
                        "trace_id": entry.correlation.trace_id,
                        "span_id": entry.correlation.span_id,
                        "parent_span_id": entry.correlation.parent_span_id
                    }

                if entry.multi_tenant:
                    log_data["multi_tenant"] = {
                        "user_id": entry.multi_tenant.user_id,
                        "session_id": entry.multi_tenant.session_id,
                        "tenant_id": entry.multi_tenant.tenant_id,
                        "organization_id": entry.multi_tenant.organization_id
                    }

                if entry.http_details:
                    log_data["http_details"] = {
                        "method": entry.http_details.method,
                        "url": entry.http_details.url,
                        "status_code": entry.http_details.status_code
                    }

                if entry.extra:
                    log_data.update(entry.extra)

                logs_data.append(log_data)

            # Send via HTTP POST
            headers = {
                'Content-Type': 'application/json',
                'DD-API-KEY': api_key
            }

            response = requests.post(intake_url, json=logs_data, headers=headers, timeout=10)

            if response.status_code in [200, 202]:
                return True
            else:
                print(f"DatadogHandler: Failed to send logs - Status {response.status_code}: {response.text}")
                return False

        except Exception as e:
            print(f"DatadogHandler: Failed to send logs - {e}")
            return False

    async def _send_to_cloud(self, entries: List[LogEntry]) -> bool:
        """Send log entries to Datadog using direct HTTP API"""
        def submit_logs():
            return self._send_logs_simple_api(entries)

        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(submit_logs)
            result = await asyncio.wrap_future(future)
            return result


class AWSCloudWatchHandler(CloudHandler):
    """AWS CloudWatch log handler"""
    
    async def _send_to_cloud(self, entries: List[LogEntry]) -> bool:
        # TODO: Implement AWS CloudWatch integration
        # This would use boto3 to send logs to CloudWatch
        print(f"AWS CloudWatch: Would send {len(entries)} log entries")
        return True


class GCPLoggingHandler(CloudHandler):
    """Google Cloud Logging handler"""
    
    async def _send_to_cloud(self, entries: List[LogEntry]) -> bool:
        # TODO: Implement GCP Logging integration
        # This would use the Google Cloud Logging client
        print(f"GCP Logging: Would send {len(entries)} log entries")
        return True


class AzureLogAnalyticsHandler(CloudHandler):
    """Azure Log Analytics handler"""
    
    async def _send_to_cloud(self, entries: List[LogEntry]) -> bool:
        # TODO: Implement Azure Log Analytics integration
        # This would use the Azure Log Analytics API
        print(f"Azure Log Analytics: Would send {len(entries)} log entries")
        return True


class DomoLogger:
    """Enhanced logger with structured logging and multiple handlers"""

    def __init__(self, config: LogConfig, app_name: str):
        self.config = config
        self.app_name = app_name
        self.handlers: List[LogHandler] = []
        self.buffer: List[LogEntry] = []
        self.correlation_manager = correlation_manager
        self.flush_task = None

        # Validate configuration
        config.validate_config()

        # Initialize handlers based on config
        self._setup_handlers()

        # Try to start background flush task if event loop is available
        try:
            asyncio.get_running_loop()
            self._start_flush_task()
        except RuntimeError:
            # No event loop, task will be started when first log is called
            pass

    def _setup_handlers(self):
        """Setup handlers based on configuration"""
        # Get handler configurations from the config
        handler_configs = self.config.get_handler_configs()
        
        for handler_config in handler_configs:
            handler_type = handler_config["type"]
            config = handler_config["config"]
            cloud_config = handler_config.get("cloud_config")
            
            if handler_type == "console":
                self.handlers.append(ConsoleHandler(config))
            elif handler_type == "file":
                self.handlers.append(FileHandler(config))
            elif handler_type == "cloud":
                if cloud_config:
                    cloud_provider = cloud_config.get("cloud_provider")
                    if cloud_provider == "datadog":
                        self.handlers.append(DatadogHandler(config))
                    elif cloud_provider == "aws":
                        self.handlers.append(AWSCloudWatchHandler(config))
                    elif cloud_provider == "gcp":
                        self.handlers.append(GCPLoggingHandler(config))
                    elif cloud_provider == "azure":
                        self.handlers.append(AzureLogAnalyticsHandler(config))
                    else:
                        raise LogConfigError(f"Unknown cloud provider: {cloud_provider}")
                else:
                    raise LogConfigError("Cloud handler missing cloud_config")
            else:
                raise LogConfigError(f"Unknown handler type: {handler_type}")

    def _start_flush_task(self):
        """Start the background flush task"""
        if self.flush_task is None:
            self.flush_task = asyncio.create_task(self._periodic_flush())

    async def log(
        self,
        level: LogLevel,
        message: str,
        **context
    ) -> bool:
        """Log a message with structured context"""

        # Check if we should log this level
        if not self.config.level.should_log(level):
            return True

        # Start flush task if not already started and event loop is available
        if self.flush_task is None:
            try:
                asyncio.get_running_loop()
                self._start_flush_task()
            except RuntimeError:
                pass

        # Create log entry
        entry = LogEntry.create(
            level=level,
            message=message,
            logger=context.get('logger', f"domolibrary.{self.app_name}"),
            user=context.get('user'),
            action=context.get('action'),
            entity=context.get('entity'),
            status=context.get('status', 'info'),
            duration_ms=context.get('duration_ms'),
            trace_id=self.correlation_manager.trace_id_var.get(),
            request_id=self.correlation_manager.request_id_var.get(),
            session_id=self.correlation_manager.session_id_var.get(),
            correlation=self.correlation_manager.correlation_var.get(),
            multi_tenant=context.get('multi_tenant'),
            http_details=context.get('http_details'),
            extra=context.get('extra', {}),
        )

        # Add to buffer
        self.buffer.append(entry)

        # Flush if buffer is full
        if len(self.buffer) >= self.config.batch_size:
            await self.flush()

        return True

    async def flush(self) -> bool:
        """Flush buffered entries to all handlers"""
        if not self.buffer:
            return True

        entries_to_flush = self.buffer.copy()
        self.buffer.clear()

        success = True
        for handler in self.handlers:
            if not await handler.write(entries_to_flush):
                success = False

        return success

    async def _periodic_flush(self):
        """Background task to periodically flush logs"""
        while True:
            await asyncio.sleep(self.config.flush_interval)
            await self.flush()

    # Convenience methods for different log levels
    async def debug(self, message: str, **context) -> bool:
        return await self.log(LogLevel.DEBUG, message, **context)

    async def info(self, message: str, **context) -> bool:
        return await self.log(LogLevel.INFO, message, **context)

    async def warning(self, message: str, **context) -> bool:
        return await self.log(LogLevel.WARNING, message, **context)

    async def error(self, message: str, **context) -> bool:
        return await self.log(LogLevel.ERROR, message, **context)

    async def critical(self, message: str, **context) -> bool:
        return await self.log(LogLevel.CRITICAL, message, **context)

    def start_request(self, parent_trace_id: Optional[str] = None, auth=None) -> str:
        """Start a new request context"""
        return self.correlation_manager.start_request(parent_trace_id, auth)

    def end_request(self):
        """End current request context"""
        # Clear context variables (they'll be reset on next request)
        pass

    async def close(self):
        """Clean up resources"""
        # Cancel flush task
        if hasattr(self, 'flush_task'):
            self.flush_task.cancel()
            try:
                await self.flush_task
            except asyncio.CancelledError:
                pass

        # Final flush
        await self.flush()

        # Close handlers
        for handler in self.handlers:
            await handler.close()


# Global logger instance
_global_logger: Optional[DomoLogger] = None

def get_logger(app_name: str = "domolibrary") -> DomoLogger:
    """Get or create the global logger instance"""
    global _global_logger
    if _global_logger is None:
        config = AzureLogAnalyticsConfig.from_env()
        _global_logger = DomoLogger(config, app_name)
    return _global_logger

def set_global_logger(logger: DomoLogger):
    """Set the global logger instance"""
    global _global_logger
    _global_logger = logger


# Logging Decorator
def log_function_call(
    action_name: Optional[str] = None,
    include_params: bool = False,
    include_result: bool = False,
    logger: Optional[DomoLogger] = None,
    log_level: LogLevel = LogLevel.INFO,
    suppress_repeated_calls: bool = True
):
    """
    Decorator to automatically log function calls with comprehensive context.

    Args:
        action_name: Custom action name, defaults to function name
        include_params: Whether to log function parameters (sanitized)
        include_result: Whether to log function result (sanitized)
        logger: Specific logger instance, defaults to global logger
        log_level: Log level for the function call
        suppress_repeated_calls: Whether to suppress logging for repeated calls (like pagination)
    """
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            if logger is None:
                log = get_logger()
            else:
                log = logger

            start_time = time.time()
            function_name = func.__qualname__

            # Check if we're already in a logged context to avoid duplicate logging
            # DISABLED FOR DEBUGGING - we want to see all logs for now
            # current_context = log.correlation_manager.get_current_context()
            # if current_context.get('trace_id') and function_name in ['get_data', 'looper']:
            #     # We're already in a logged context, just execute the function
            #     return await func(*args, **kwargs)

            # Check for pagination requests (simplified detection)
            pagination_context = None

            if suppress_repeated_calls and function_name in ['get_data', 'looper']:
                # Extract URL from kwargs for pagination detection
                url = kwargs.get('url', '')
                current_context = log.correlation_manager.get_current_context()
                current_trace_id = current_context.get('trace_id')

                # Check if this is a dataset query pagination request
                is_pagination_request = (
                    '/api/query/v1/execute' in url or  # Query execute endpoint (most common)
                    '/data/v1/query' in url or  # Dataset query endpoint
                    ('offset=' in url) or  # Any URL with offset pagination
                    ('skip=' in url)  # Any URL with skip pagination
                )

                if is_pagination_request:
                    # Track pagination requests by trace_id and base URL
                    url_base = url.split('?')[0] if '?' in url else url
                    
                    # Initialize or update pagination tracking
                    if not hasattr(log, '_pagination_requests'):
                        log._pagination_requests = {}
                    
                    pagination_key = f"{current_trace_id}_{url_base}"
                    if pagination_key in log._pagination_requests:
                        # This is a continued pagination request
                        log._pagination_requests[pagination_key]['count'] += 1
                        pagination_context = {
                            'is_pagination': True,
                            'request_count': log._pagination_requests[pagination_key]['count'],
                            'url_base': url_base
                        }
                    else:
                        # First request in this pagination sequence
                        log._pagination_requests[pagination_key] = {
                            'count': 1,
                            'start_time': time.time(),
                            'url_base': url_base
                        }
                        pagination_context = {
                            'is_pagination': False,
                            'request_count': 1,
                            'url_base': url_base
                        }

            # Execute the function
            result = await func(*args, **kwargs)

            # Get caller context
            caller_frame = inspect.currentframe().f_back
            caller_info = {
                "file": caller_frame.f_code.co_filename,
                "line": caller_frame.f_lineno,
                "function": caller_frame.f_code.co_name
            }

            # Extract auth and entity info from kwargs
            auth = kwargs.get('auth')
            url = kwargs.get('url', '')
            method = kwargs.get('method', 'GET')
            headers = kwargs.get('headers', {})
            body = kwargs.get('body')

            # Extract entity information using the helper function
            entity = extract_entity_from_args(args, kwargs)

            # Start request context (inherit trace_id from parent if available)
            request_id = log.start_request(auth=auth)

            # Create HTTP details object with all request information
            params = kwargs.get('params')
            http_details = HTTPDetails(
                method=method,
                url=url,
                headers=headers,
                params=params,
                request_body=body
            )

            # Get current correlation context
            correlation_context = log.correlation_manager.get_current_context()
            correlation = correlation_context.get('correlation')
            if correlation:
                correlation_obj = Correlation(**correlation)
            else:
                correlation_obj = None

            # Create multi-tenant context - prioritize auth object's context
            multi_tenant = None
            if auth:
                # Extract user information from auth object
                user_id = getattr(auth, 'user_id', None)
                user_name = getattr(auth, 'user_name', None) or getattr(auth, 'username', None) or getattr(auth, 'domo_username', None)

                # Extract tenant/organization information - always use domo_instance if available
                domo_instance = getattr(auth, 'domo_instance', None)
                tenant_id = domo_instance
                organization_id = domo_instance

                # Extract session information - prioritize generating from auth
                if user_id and domo_instance:
                    session_id = f"{domo_instance}_{user_id}"
                elif user_name and domo_instance:
                    # Use username/email for session ID if user_id not available
                    session_id = f"{domo_instance}_{user_name}"
                else:
                    session_id = getattr(auth, 'session_id', None) or log.correlation_manager.session_id_var.get()

                # Always create multi-tenant object if we have auth (at minimum we have domo_instance)
                # Ensure consistent tenant/organization info across all logs
                if hasattr(auth, 'domo_instance') or user_id or user_name or session_id:
                    multi_tenant = MultiTenant(
                        user_id=user_id,
                        session_id=session_id,
                        tenant_id=tenant_id,
                        organization_id=organization_id
                    )
            else:
                # Even without auth, try to get session_id from context
                session_id = log.correlation_manager.session_id_var.get()
                if session_id:
                    multi_tenant = MultiTenant(
                        user_id=None,
                        session_id=session_id,
                        tenant_id=None,
                        organization_id=None
                    )

            # Prepare function context
            func_context = {
                "action": action_name or function_name,
                "entity": entity,
                "correlation": correlation_obj,
                "multi_tenant": multi_tenant,
                "http_details": http_details,
                # Don't set user directly - it's in multi_tenant if available
            }
            

            # Add function and caller info to extra for debugging
            extra_func_info = {
                "function": function_name,
                "module": func.__module__,
                "caller": caller_info,
            }

            # Add pagination context if available
            if pagination_context:
                extra_func_info["pagination"] = pagination_context
            
            # Don't duplicate session_id and user_id in extra - they're already in multi_tenant
            # Only add session_id and user_id to extra if multi_tenant is not available
            if not multi_tenant:
                extra_func_info["session_id"] = log.correlation_manager.session_id_var.get()
                # Try to extract user_id from auth if available
                if auth:
                    user_id = getattr(auth, 'user_id', None)
                    if user_id:
                        extra_func_info["user_id"] = user_id

            # Include sanitized parameters if requested
            if include_params:
                # Sanitize sensitive parameters
                safe_kwargs = {}
                for k, v in kwargs.items():
                    if k in ['password', 'token', 'auth_token', 'access_token']:
                        safe_kwargs[k] = "***"
                    elif k == 'auth':
                        safe_kwargs[k] = str(type(v).__name__)
                    elif isinstance(v, (str, int, float, bool, type(None))):
                        safe_kwargs[k] = v
                    else:
                        safe_kwargs[k] = f"<{type(v).__name__}>"

                func_context["parameters"] = safe_kwargs

            try:
                # Call the function
                result = await func(*args, **kwargs)

                # Calculate duration
                duration_ms = int((time.time() - start_time) * 1000)

                # Update HTTP details with response information if result is ResponseGetData
                is_http_error = False
                if hasattr(result, 'status') and hasattr(result, 'response') and http_details:
                    status_code = getattr(result, 'status', None)
                    http_details.status_code = status_code
                    
                    # Check if this is an HTTP error status
                    if status_code and status_code >= 400:
                        is_http_error = True
                    
                    response_data = getattr(result, 'response', None)
                    if response_data is not None:
                        if isinstance(response_data, (str, bytes)):
                            http_details.response_size = len(response_data)
                            # Truncate large responses for logging
                            if len(str(response_data)) > 500:
                                http_details.response_body = str(response_data)[:500] + "..."
                            else:
                                http_details.response_body = str(response_data)
                        elif hasattr(response_data, '__len__'):
                            try:
                                response_length = len(response_data)
                                http_details.response_size = response_length
                                if response_length > 100:
                                    http_details.response_body = f"<{type(response_data).__name__} with {response_length} items>"
                                else:
                                    http_details.response_body = f"<{type(response_data).__name__}>"
                            except:
                                http_details.response_body = f"<{type(response_data).__name__}>"
                        else:
                            http_details.response_body = f"<{type(response_data).__name__}>"

                # Prepare result context - check for HTTP errors
                result_context = {
                    "duration_ms": duration_ms,
                    "status": "error" if is_http_error else "success",
                }

                if include_result and result is not None:
                    if hasattr(result, '__len__') and len(result) > 100:
                        result_context["result"] = f"<{type(result).__name__} with {len(result)} items>"
                    elif isinstance(result, (str, int, float, bool, type(None))):
                        result_context["result"] = result
                    else:
                        result_context["result"] = f"<{type(result).__name__}>"

                # Update func_context with potentially updated http_details
                func_context["http_details"] = http_details

                # Log based on whether this is an HTTP error or success
                if is_http_error:
                    await log.error(
                        f"Function {function_name} completed with HTTP error {status_code}",
                        logger=f"domolibrary.{func.__module__}",
                        **func_context,
                        **result_context,
                        extra=extra_func_info
                    )
                else:
                    await log.info(
                        f"Function {function_name} completed successfully",
                        logger=f"domolibrary.{func.__module__}",
                        **func_context,
                        **result_context,
                        extra=extra_func_info
                    )

                return result

            except Exception as e:
                # Calculate duration
                duration_ms = int((time.time() - start_time) * 1000)

                # If we have an exception that contains response info (like DomoError), capture it
                if hasattr(e, 'status') and http_details:
                    http_details.status_code = getattr(e, 'status', None)
                elif hasattr(e, 'response') and http_details:
                    response_data = getattr(e, 'response', None)
                    if hasattr(response_data, 'status_code'):
                        http_details.status_code = response_data.status_code

                # Update func_context with potentially updated http_details
                func_context["http_details"] = http_details

                # Log failed function call
                error_extra = {
                    **extra_func_info,
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "async_call": True
                }
                await log.error(
                    f"Function {function_name} failed: {str(e)}",
                    logger=f"domolibrary.{func.__module__}",
                    **func_context,
                    duration_ms=duration_ms,
                    status="error",
                    extra=error_extra
                )
                raise
            finally:
                log.end_request()

        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            if logger is None:
                log = get_logger()
            else:
                log = logger

            start_time = time.time()
            function_name = func.__qualname__

            # Check if we're already in a logged context to avoid duplicate logging
            # DISABLED FOR DEBUGGING - we want to see all logs for now
            # current_context = log.correlation_manager.get_current_context()
            # if current_context.get('trace_id') and function_name in ['get_data', 'looper']:
            #     # We're already in a logged context, just execute the function
            #     return func(*args, **kwargs)

            # Get caller context
            caller_frame = inspect.currentframe().f_back
            caller_info = {
                "file": caller_frame.f_code.co_filename,
                "line": caller_frame.f_lineno,
                "function": caller_frame.f_code.co_name
            }

            # Extract auth and entity info from kwargs
            auth = kwargs.get('auth')
            url = kwargs.get('url', '')
            method = kwargs.get('method', 'GET')
            headers = kwargs.get('headers', {})
            body = kwargs.get('body')

            # Extract entity information using the helper function
            entity = extract_entity_from_args(args, kwargs)

            # Start request context (inherit trace_id from parent if available)
            request_id = log.start_request(auth=auth)

            # Create HTTP details object with all request information
            params = kwargs.get('params')
            http_details = HTTPDetails(
                method=method,
                url=url,
                headers=headers,
                params=params,
                request_body=body
            )

            # Get current correlation context
            correlation_context = log.correlation_manager.get_current_context()
            correlation = correlation_context.get('correlation')
            if correlation:
                correlation_obj = Correlation(**correlation)
            else:
                correlation_obj = None

            # Create multi-tenant context (renamed to auth_context for clarity)
            multi_tenant = None
            if auth:
                # Extract user information from auth object
                user_id = getattr(auth, 'user_id', None)
                user_name = getattr(auth, 'user_name', None) or getattr(auth, 'username', None) or getattr(auth, 'domo_username', None)

                # Extract tenant/organization information - always use domo_instance if available
                domo_instance = getattr(auth, 'domo_instance', None)
                tenant_id = domo_instance
                organization_id = domo_instance

                # Extract session information - prioritize generating from auth
                if user_id and domo_instance:
                    session_id = f"{domo_instance}_{user_id}"
                elif user_name and domo_instance:
                    # Use username/email for session ID if user_id not available
                    session_id = f"{domo_instance}_{user_name}"
                else:
                    session_id = getattr(auth, 'session_id', None) or log.correlation_manager.session_id_var.get()

                # Always create multi-tenant object if we have auth (at minimum we have domo_instance)
                # Ensure consistent tenant/organization info across all logs
                if hasattr(auth, 'domo_instance') or user_id or user_name or session_id:
                    multi_tenant = MultiTenant(
                        user_id=user_id,
                        session_id=session_id,
                        tenant_id=tenant_id,
                        organization_id=organization_id
                    )
            else:
                # Even without auth, try to get session_id from context
                session_id = log.correlation_manager.session_id_var.get()
                if session_id:
                    multi_tenant = MultiTenant(
                        user_id=None,
                        session_id=session_id,
                        tenant_id=None,
                        organization_id=None
                    )

            # Prepare function context
            func_context = {
                "action": action_name or function_name,
                "entity": entity,
                "correlation": correlation_obj,
                "multi_tenant": multi_tenant,
                "http_details": http_details,
                # Don't set user directly - it's in multi_tenant if available
            }

            # Add function and caller info to extra for debugging
            extra_func_info = {
                "function": function_name,
                "module": func.__module__,
                "caller": caller_info,
            }

            # Add pagination context if available (Note: sync wrapper doesn't have pagination tracking)
            # Pagination context is only available in async wrapper
            
            # Don't duplicate session_id and user_id in extra - they're already in multi_tenant
            # Only add session_id and user_id to extra if multi_tenant is not available
            if not multi_tenant:
                extra_func_info["session_id"] = log.correlation_manager.session_id_var.get()
                # Try to extract user_id from auth if available
                if auth:
                    user_id = getattr(auth, 'user_id', None)
                    if user_id:
                        extra_func_info["user_id"] = user_id

            try:
                # Call the function
                result = func(*args, **kwargs)

                # Calculate duration
                duration_ms = int((time.time() - start_time) * 1000)

                # Update HTTP details with response information if result is ResponseGetData
                is_http_error = False
                if hasattr(result, 'status') and hasattr(result, 'response') and http_details:
                    status_code = getattr(result, 'status', None)
                    http_details.status_code = status_code
                    
                    # Check if this is an HTTP error status
                    if status_code and status_code >= 400:
                        is_http_error = True
                    
                    response_data = getattr(result, 'response', None)
                    if response_data is not None:
                        if isinstance(response_data, (str, bytes)):
                            http_details.response_size = len(response_data)
                            # Truncate large responses for logging
                            if len(str(response_data)) > 500:
                                http_details.response_body = str(response_data)[:500] + "..."
                            else:
                                http_details.response_body = str(response_data)
                        elif hasattr(response_data, '__len__'):
                            try:
                                response_length = len(response_data)
                                http_details.response_size = response_length
                                if response_length > 100:
                                    http_details.response_body = f"<{type(response_data).__name__} with {response_length} items>"
                                else:
                                    http_details.response_body = f"<{type(response_data).__name__}>"
                            except:
                                http_details.response_body = f"<{type(response_data).__name__}>"
                        else:
                            http_details.response_body = f"<{type(response_data).__name__}>"

                # Prepare result context - check for HTTP errors
                result_context = {
                    "duration_ms": duration_ms,
                    "status": "error" if is_http_error else "success",
                }

                # Update func_context with potentially updated http_details
                func_context["http_details"] = http_details

                # Log based on whether this is an HTTP error or success
                sync_extra = {
                    **extra_func_info,
                    "async_call": False
                }
                
                if is_http_error:
                    entry = LogEntry.create(
                        level=LogLevel.ERROR,
                        message=f"Function {function_name} completed with HTTP error {status_code}",
                        logger=f"domolibrary.{func.__module__}",
                        **func_context,
                        **result_context,
                        extra=sync_extra
                    )
                else:
                    entry = LogEntry.create(
                        level=LogLevel.INFO,
                        message=f"Function {function_name} completed successfully",
                        logger=f"domolibrary.{func.__module__}",
                        **func_context,
                        **result_context,
                        extra=sync_extra
                    )

                # Add to buffer and flush synchronously
                log.buffer.append(entry)

                # Always try to print immediately in sync context if no event loop
                try:
                    asyncio.get_running_loop()
                    # Event loop exists, flush will happen on next async call
                    if len(log.buffer) >= log.config.batch_size:
                        pass  # Will be flushed by background task
                except RuntimeError:
                    # No event loop, print directly
                    if log.config.pretty_print:
                        print(json.dumps(entry.to_dict(), indent=2, default=str))
                        print("-" * 80)
                    else:
                        print(entry.to_json())
                    log.buffer.clear()  # Clear buffer after printing

                return result

            except Exception as e:
                # Calculate duration
                duration_ms = int((time.time() - start_time) * 1000)

                # If we have an exception that contains response info (like DomoError), capture it
                if hasattr(e, 'status') and http_details:
                    http_details.status_code = getattr(e, 'status', None)
                elif hasattr(e, 'response') and http_details:
                    response_data = getattr(e, 'response', None)
                    if hasattr(response_data, 'status_code'):
                        http_details.status_code = response_data.status_code

                # Update func_context with potentially updated http_details
                func_context["http_details"] = http_details

                # Log failed function call
                # For sync functions, we need to create the log entry manually since we can't await
                sync_error_extra = {
                    **extra_func_info,
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "async_call": False
                }
                error_entry = LogEntry.create(
                    level=LogLevel.ERROR,
                    message=f"Function {function_name} failed: {str(e)}",
                    logger=f"domolibrary.{func.__module__}",
                    **func_context,
                    duration_ms=duration_ms,
                    status="error",
                    extra=sync_error_extra
                )

                # Add to buffer and flush synchronously
                log.buffer.append(error_entry)
                try:
                    asyncio.get_running_loop()
                    # Event loop exists, flush will happen on next async call
                    pass
                except RuntimeError:
                    # No event loop, print directly
                    if log.config.pretty_print:
                        print(json.dumps(error_entry.to_dict(), indent=2, default=str))
                        print("-" * 80)
                    else:
                        print(error_entry.to_json())
                    log.buffer.clear()  # Clear buffer after printing
                raise
            finally:
                log.end_request()

        # Return appropriate wrapper based on whether function is async
        if inspect.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper

    return decorator
