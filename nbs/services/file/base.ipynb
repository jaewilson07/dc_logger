{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96ff75e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp logs.services.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7d094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from typing import Optional,Literal\n",
    "\n",
    "from dc_logger.client.base import ServiceHandler, ServiceConfig, LogEntry\n",
    "from dc_logger.client.exceptions import LogConfigError, LogHandlerError,LogWriteError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50c85e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "from dc_logger.client.base import ServiceConfig, OutputMode\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e4d82d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class File_ServiceConfig(ServiceConfig):\n",
    "    \"\"\"Configuration for file-based logging output\"\"\"\n",
    "    destination: str\n",
    "    output_mode: Literal[\"file\"] = \"file\"\n",
    "    format: Literal[\"json\", \"text\", \"csv\"] = \"text\"\n",
    "    append: bool = True  # optional flag for overwrite or append\n",
    "\n",
    "    def validate_config(self) -> bool:\n",
    "        if not self.destination:\n",
    "            raise LogConfigError(\"File destination must be provided.\")\n",
    "        if self.format not in (\"json\", \"text\", \"csv\"):\n",
    "            raise LogConfigError(f\"Unsupported file format: {self.format}\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5909f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from dc_logger.client.base import Handler_BufferSettings\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class FileHandler(ServiceHandler):\n",
    "    \"\"\"Handler for file output\"\"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize file handler after dataclass initialization\"\"\"\n",
    "        self.service_config.validate_config()\n",
    "        self.file_path = self.service_config.destination\n",
    "        self.append_mode = \"a\" if self.service_config.append else \"w\"\n",
    "        self._ensure_directory_exists()\n",
    "\n",
    "    def _ensure_directory_exists(self):\n",
    "        \"\"\" Create parent directories in case they don't exist.\"\"\"\n",
    "        file_dir = os.path.dirname(self.file_path)\n",
    "        if not file_dir:\n",
    "            return\n",
    "        try:\n",
    "            os.makedirs(file_dir, exist_ok=True)\n",
    "        except PermissionError as e:\n",
    "            raise LogHandlerError(f\"Permissions denied creating directory {file_dir}: {e}\")\n",
    "        except OSError as e:\n",
    "            raise LogHandlerError(f\"OS error creating the directory {file_dir}: {e}\")\n",
    "            \n",
    "    async def _write_json(self, entry: LogEntry) -> bool:\n",
    "        try:\n",
    "            with open(self.file_path, self.append_mode, encoding=\"utf-8\") as f:\n",
    "                json.dump(entry.to_dict(), f, ensure_ascii=False)\n",
    "                f.write(\"\\n\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            raise LogWriteError(f\"Error writing JSON to file {self.file_path}: {e}\")\n",
    "\n",
    "    async def _write_text(self, entry: LogEntry) -> bool:\n",
    "        try:\n",
    "            # Get all fields from the entry\n",
    "            entry_dict = entry.to_dict()\n",
    "            \n",
    "            # Build the main log line\n",
    "            level = entry.level.value if hasattr(entry.level, 'value') else entry.level\n",
    "            line = f\"[{entry.timestamp}] {level} - {entry.message}\"\n",
    "            \n",
    "            # Collect metadata (all fields except timestamp, level, and message)\n",
    "            metadata_parts = []\n",
    "            for key, value in entry_dict.items():\n",
    "                if key not in ['timestamp', 'level', 'message']:\n",
    "                    if isinstance(value, dict):\n",
    "                        # Format nested dicts\n",
    "                        nested_parts = [f\"{key}.{k}={v}\" for k, v in value.items() if v is not None]\n",
    "                        metadata_parts.extend(nested_parts)\n",
    "                    elif value is not None:\n",
    "                        metadata_parts.append(f\"{key}={value}\")\n",
    "            \n",
    "            # Add metadata if present\n",
    "            if metadata_parts:\n",
    "                line += \" | \" + \", \".join(metadata_parts)\n",
    "            \n",
    "            line += \"\\n\"\n",
    "            \n",
    "            with open(self.file_path, self.append_mode, encoding=\"utf-8\") as f:\n",
    "                f.write(line)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            raise LogWriteError(f\"Error writing text to file {self.file_path}: {e}\")\n",
    "\n",
    "    def _flatten_dict(self, d: dict, parent_key: str = '', sep: str = '.') -> dict:\n",
    "        \"\"\"Flatten nested dictionaries into dot notation\"\"\"\n",
    "        items = []\n",
    "        for k, v in d.items():\n",
    "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "            if isinstance(v, dict) and v:\n",
    "                items.extend(self._flatten_dict(v, new_key, sep=sep).items())\n",
    "            else:\n",
    "                items.append((new_key, v))\n",
    "        return dict(items)\n",
    "    \n",
    "    async def _write_csv(self, entry: LogEntry) -> bool:\n",
    "        try:\n",
    "            file_exists = os.path.exists(self.file_path)\n",
    "            entry_dict = entry.to_dict()\n",
    "            \n",
    "            # Flatten the entry dictionary to handle nested objects\n",
    "            flattened_entry = self._flatten_dict(entry_dict)\n",
    "            \n",
    "            # Get existing fieldnames if file exists\n",
    "            existing_fieldnames = []\n",
    "            if file_exists and os.path.getsize(self.file_path) > 0:\n",
    "                with open(self.file_path, 'r', newline=\"\", encoding=\"utf-8\") as f:\n",
    "                    reader = csv.DictReader(f)\n",
    "                    existing_fieldnames = reader.fieldnames or []\n",
    "            \n",
    "            # Merge existing fieldnames with new ones, preserving order\n",
    "            # Priority order: timestamp, level, app_name, message first\n",
    "            priority_fields = [\"timestamp\", \"level\", \"app_name\", \"message\"]\n",
    "            new_fields = set(flattened_entry.keys())\n",
    "            \n",
    "            # Start with priority fields that exist\n",
    "            fieldnames = [f for f in priority_fields if f in new_fields]\n",
    "            \n",
    "            # Add existing fields that aren't priority fields\n",
    "            for field in existing_fieldnames:\n",
    "                if field not in fieldnames:\n",
    "                    fieldnames.append(field)\n",
    "            \n",
    "            # Add new fields that aren't already in the list\n",
    "            for field in sorted(new_fields - set(fieldnames)):\n",
    "                fieldnames.append(field)\n",
    "            \n",
    "            # Determine if we need to rewrite the file (new columns added)\n",
    "            needs_rewrite = file_exists and set(fieldnames) != set(existing_fieldnames)\n",
    "            \n",
    "            if needs_rewrite:\n",
    "                # Read existing data\n",
    "                existing_data = []\n",
    "                with open(self.file_path, 'r', newline=\"\", encoding=\"utf-8\") as f:\n",
    "                    reader = csv.DictReader(f)\n",
    "                    existing_data = list(reader)\n",
    "                \n",
    "                # Rewrite file with new headers\n",
    "                with open(self.file_path, 'w', newline=\"\", encoding=\"utf-8\") as f:\n",
    "                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                    writer.writeheader()\n",
    "                    for row in existing_data:\n",
    "                        writer.writerow(row)\n",
    "                    writer.writerow(flattened_entry)\n",
    "            else:\n",
    "                # Normal append or new file\n",
    "                with open(self.file_path, self.append_mode, newline=\"\", encoding=\"utf-8\") as f:\n",
    "                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                    if not file_exists or os.path.getsize(self.file_path) == 0:\n",
    "                        writer.writeheader()\n",
    "                    writer.writerow(flattened_entry)\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            raise LogWriteError(f\"Error writing CSV to file {self.file_path}: {e}\")\n",
    "\n",
    "    async def write(self, entries: List[LogEntry]) -> bool:\n",
    "        \"\"\"Write log entries to the file\"\"\"\n",
    "        if not isinstance(entries, list):\n",
    "            entries = [entries]\n",
    "        \n",
    "        try:\n",
    "            output_format = self.service_config.format\n",
    "\n",
    "            for entry in entries:\n",
    "                if output_format == \"json\":\n",
    "                    await self._write_json(entry)\n",
    "                elif output_format == \"csv\":\n",
    "                    await self._write_csv(entry)\n",
    "                elif output_format == \"text\":\n",
    "                    await self._write_text(entry)\n",
    "                else:\n",
    "                    raise LogConfigError(f\"Unsupported output format: {output_format}\")\n",
    "            \n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            raise LogWriteError(f\"Error writing to file {self.file_path}: {e}\")\n",
    "\n",
    "    async def flush(self) -> bool:\n",
    "        \"\"\"File writes are immediate; no flush needed\"\"\"\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d9ce494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; \n",
    "nbdev.nbdev_export('./base.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
